## 机器学习（入门）——线性回归

前言：作为初学者了解到 机器学习目前分为监督学习和非监督学习，其中监督学习算法分为分类算法和回归算法两种，主要是依据其类别标签分布类型为离散型、连续型而定义的。分类算法用于离散型分布预测，如KNN、决策树、SVM、Logistic回归等。回归算法用于连续性分布预测，针对数值型样本使用回归能够在给定输入时预测出相对较为准确的数值。

#### 回归

回归应用于监督学习中，简单点来理解，就是给定一个点集D，用一个函数去拟合该点集，并且使得点集于拟合函数简的误差最小。回归分析主要包括线性回归和非线性回归。

#### 线性回归

线性回归是指通过属性或者特征变量的线性组合来进行预测的线性模型，目的在于找到一条直线或者平面或者高维的超平面，使得预测值与真实值之间的误差最小化。如果将属性先映射到一个函数再参与线性计算，则可以表达出属性与结果间的非线性关系。多元线性回归原理与其类似。

这里用x1,x2..xn来表示特征空间中的分量，则可以得到一个估计函数：![clip_image004](http://images.cnblogs.com/cnblogs_com/jerrylead/201103/201103052209094015.png)

θ可以理解为对每一个分量的影响力，即权重，期望学得的函数可以变换为：![clip_image005](http://images.cnblogs.com/cnblogs_com/jerrylead/201103/201103052209103916.png)

预测值与真实值之间总是会存在差异的，所以我们需要对h函数进行评估，一般此类函数称为损失函数（loss function）或者代价函数（error function），这里采用以下函数表示：![clip_image006](http://images.cnblogs.com/cnblogs_com/jerrylead/201103/201103052209117372.png)

其采用的是对x(i)的估计值和真实值y(i)的差的平方和作为损失函数，而机器学习的最终目的都是为了使损失函数达到最小值，从而提高拟合程度，这种方法有很多种，如最小二乘法和梯度下降法等。

 **1： 函数模型（Model）：**

​                 **![img](https://images0.cnblogs.com/blog2015/633472/201503/262037556613399.jpg)**

​           **假设有训练数据**

​                **![img](https://images0.cnblogs.com/blog2015/633472/201503/262041198028564.jpg)**

​          **那么为了方便我们写成矩阵的形式**

​                **![img](https://images0.cnblogs.com/blog2015/633472/201503/262042295678545.jpg)**

------

#### 简单的线性回归

代码示例：

```
import numpy as np
from matplotlib import pylab as pl

#定义训练数据

x = np.array([1,3,2,1,3])
y = np.array([14,24,18,17,27])

#回归方程求取函数

def fit(x,y):
    if len(x) != len(y):
        return
    numerator = 0.0
    denominator = 0.0
    x_mean = np.mean(x)
    y_mean = np.mean(y)
    for i in range(len(x)):
        numerator += (x[i]-x_mean)*(y[i]-y_mean)
        denominator += np.square((x[i]-x_mean))
    b0 = numerator/denominator
    b1 = y_mean - b0*x_mean
    return b0,b1

#定义预测函数

def predit(x,b0,b1):
    return b0*x + b1

#求取回归方程

b0,b1 = fit(x,y)
print('Line is:y = %2.0fx + %2.0f'%(b0,b1))

#预测

x_test = np.array([0.5,1.5,2.5,3,4])
y_test = np.zeros((1,len(x_test)))
for i in range(len(x_test)):
    y_test[0][i] = predit(x_test[i],b0,b1)

#绘制图像

xx = np.linspace(0, 5)
yy = b0*xx + b1
pl.plot(xx,yy,'k-')
pl.scatter(x,y,cmap=pl.cm.Paired)
pl.scatter(x_test,y_test[0],cmap=pl.cm.Paired)
pl.show()
```

输出：

```
Line is:y =  5x + 10
```

![img](https://images2018.cnblogs.com/blog/1358931/201804/1358931-20180424235927418-1173120419.png)



其中蓝色为测试数据，橙色为预测数据，代码示例为转载，目的在于展示线性回归过程及预测过程

